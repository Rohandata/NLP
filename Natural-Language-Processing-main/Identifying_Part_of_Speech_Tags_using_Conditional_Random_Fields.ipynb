{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Identifying Part of Speech Tags using Conditional Random Fields.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIiqHmq3tJfpQoTQw/3IGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysaikiran2208/Natural-Language-Processing/blob/main/Identifying_Part_of_Speech_Tags_using_Conditional_Random_Fields.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL222OnWlvYz"
      },
      "source": [
        "The Different POS Tagging Techniques\n",
        "There are different techniques for POS Tagging:\n",
        "\n",
        "Lexical Based Methods — Assigns the POS tag the most frequently occurring with a word in the training corpus.\n",
        "\n",
        "Rule-Based Methods — Assigns POS tags based on rules. For example, we can have a rule that says, words ending with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.\n",
        "\n",
        "Probabilistic Methods — This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
        "\n",
        "Deep Learning Methods — Recurrent Neural Networks can also be used for POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCN_ztPSuATb",
        "outputId": "2ecf800a-075e-4c52-f1ea-a047c43325a4"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "!pip install scorm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n",
            "Collecting scorm\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1d/ea8e1aeacb671eb70a460c2330e483d10d47cba812cb2afc5d2578567bd7/scorm-0.0.1.tar.gz\n",
            "Building wheels for collected packages: scorm\n",
            "  Building wheel for scorm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scorm: filename=scorm-0.0.1-cp37-none-any.whl size=1182 sha256=6692d84c78fb46be10fc685f19117c92bb2848ee3ab5282889023f36be053fcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/a0/12/912e90207762af12ea92758fbb51b664eaa62634808a0a4640\n",
            "Successfully built scorm\n",
            "Installing collected packages: scorm\n",
            "Successfully installed scorm-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYfjPH3xazH_"
      },
      "source": [
        "import nltk, re, pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pprint, time\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "from collections import Counter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP9XADIkw9Bh",
        "outputId": "4fc9e48f-23ea-42d3-932a-73de59ca0edb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McJbrGDxxJg4",
        "outputId": "2916d5fc-70f5-4de7-8250-8e33b8da1f10"
      },
      "source": [
        "\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2GEdf8eoniG"
      },
      "source": [
        "\n",
        "Introduction\n",
        "\n",
        "In this work, we will use CRF Classifier for POS Tagging. The dataset we will use in the PennTree Bank Corpus, with the universal Tag Set. This tag set has 12 unique POS Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQgbxpFw2BS"
      },
      "source": [
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrHpFVHBxBfg",
        "outputId": "936fd5e9-fbe5-4034-d349-c5f9e315666a"
      },
      "source": [
        "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
        "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
        "print(\"Total Number of Tagged words\", len(tagged_words))\n",
        "vocab=set([word for word,tag in tagged_words])\n",
        "print(\"Vocabulary of the Corpus\",len(vocab))\n",
        "tags=set([tag for word,tag in tagged_words])\n",
        "print(\"Number of Tags in the Corpus \",len(tags))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Tagged Sentences  3914\n",
            "Total Number of Tagged words 100676\n",
            "Vocabulary of the Corpus 12408\n",
            "Number of Tags in the Corpus  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0S6x9YgohI3"
      },
      "source": [
        "#Splitting Data into train and test set - 80-20 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26UwWkK43qRL",
        "outputId": "8f1a3ed8-418d-4a2a-82f0-c4898d6236f1"
      },
      "source": [
        "train_set, test_set = train_test_split(tagged_sentence,test_size=0.2,random_state=1234)\n",
        "print(\"Number of Sentences in Training Data \",len(train_set))\n",
        "print(\"Number of Sentences in Testing Data \",len(test_set))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences in Training Data  3131\n",
            "Number of Sentences in Testing Data  783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csJ2zAV2orq3"
      },
      "source": [
        "\n",
        "Define the feature function. The following features can be used\n",
        "\n",
        "1.Is the first letter capitalised.\n",
        "\n",
        "2.Is it the first word in the sentence?\n",
        "\n",
        "3.Is it the last word?\n",
        "\n",
        "4.What is the prefix of the word?\n",
        "\n",
        "5.What is the suffix of the word?\n",
        "\n",
        "6.Is the complete word captilised?\n",
        "\n",
        "7.What is the previous word?\n",
        "\n",
        "8.What is the next word?\n",
        "\n",
        "9.Is it numeric?\n",
        "\n",
        "10.Is it alphanumeric?\n",
        "\n",
        "11.Is there an hyphen in the word?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0X0MYH_317g"
      },
      "source": [
        "def features(sentence,index):\n",
        "    ### sentence is of the form [w1,w2,w3,..], index is the position of the word in the sentence\n",
        "    return {\n",
        "        'is_first_capital':int(sentence[index][0].isupper()),\n",
        "        'is_first_word': int(index==0),\n",
        "        'is_last_word':int(index==len(sentence)-1),\n",
        "        'is_complete_capital': int(sentence[index].upper()==sentence[index]),\n",
        "        'prev_word':'' if index==0 else sentence[index-1],\n",
        "        'next_word':'' if index==len(sentence)-1 else sentence[index+1],\n",
        "        'is_numeric':int(sentence[index].isdigit()),\n",
        "        'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "        'prefix_1':sentence[index][0],\n",
        "        'prefix_2': sentence[index][:2],\n",
        "        'prefix_3':sentence[index][:3],\n",
        "        'prefix_4':sentence[index][:4],\n",
        "        'suffix_1':sentence[index][-1],\n",
        "        'suffix_2':sentence[index][-2:],\n",
        "        'suffix_3':sentence[index][-3:],\n",
        "        'suffix_4':sentence[index][-4:],\n",
        "        'word_has_hyphen': 1 if '-' in sentence[index] else 0\n",
        "        \n",
        "        \n",
        "    }"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIIhEQKpo85n"
      },
      "source": [
        "Need to seperate labels and the sentences in both training and test data,for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbQGxGJwoVM0"
      },
      "source": [
        "def untag(sentence):\n",
        "    return [word for word,tag in sentence]\n",
        "\n",
        "\n",
        "def prepareData(tagged_sentences):\n",
        "    X,y=[],[]\n",
        "    for sentences in tagged_sentences:\n",
        "        X.append([features(untag(sentences), index) for index in range(len(sentences))])\n",
        "        y.append([tag for word,tag in sentences])\n",
        "    return X,y"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSJCAGO65VUv",
        "outputId": "ee29d7c6-8df9-4fcf-ed1f-0fbd5f30692a"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ADP',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'NOUN',\n",
              " 'CONJ',\n",
              " 'NOUN',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " 'ADJ',\n",
              " 'NOUN',\n",
              " '.',\n",
              " 'X',\n",
              " 'VERB',\n",
              " 'NUM',\n",
              " 'DET',\n",
              " 'ADV',\n",
              " 'ADV',\n",
              " 'PRON',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " 'NOUN',\n",
              " 'X',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICTNyY4v5PeW",
        "outputId": "bf4f647f-90de-45ee-ca78-514d136ed96f"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 1,\n",
              "  'is_first_word': 1,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'Wall',\n",
              "  'prefix_1': 'O',\n",
              "  'prefix_2': 'On',\n",
              "  'prefix_3': 'On',\n",
              "  'prefix_4': 'On',\n",
              "  'prev_word': '',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'On',\n",
              "  'suffix_3': 'On',\n",
              "  'suffix_4': 'On',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 1,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'Street',\n",
              "  'prefix_1': 'W',\n",
              "  'prefix_2': 'Wa',\n",
              "  'prefix_3': 'Wal',\n",
              "  'prefix_4': 'Wall',\n",
              "  'prev_word': 'On',\n",
              "  'suffix_1': 'l',\n",
              "  'suffix_2': 'll',\n",
              "  'suffix_3': 'all',\n",
              "  'suffix_4': 'Wall',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 1,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'men',\n",
              "  'prefix_1': 'S',\n",
              "  'prefix_2': 'St',\n",
              "  'prefix_3': 'Str',\n",
              "  'prefix_4': 'Stre',\n",
              "  'prev_word': 'Wall',\n",
              "  'suffix_1': 't',\n",
              "  'suffix_2': 'et',\n",
              "  'suffix_3': 'eet',\n",
              "  'suffix_4': 'reet',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'and',\n",
              "  'prefix_1': 'm',\n",
              "  'prefix_2': 'me',\n",
              "  'prefix_3': 'men',\n",
              "  'prefix_4': 'men',\n",
              "  'prev_word': 'Street',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'men',\n",
              "  'suffix_4': 'men',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'women',\n",
              "  'prefix_1': 'a',\n",
              "  'prefix_2': 'an',\n",
              "  'prefix_3': 'and',\n",
              "  'prefix_4': 'and',\n",
              "  'prev_word': 'men',\n",
              "  'suffix_1': 'd',\n",
              "  'suffix_2': 'nd',\n",
              "  'suffix_3': 'and',\n",
              "  'suffix_4': 'and',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'walk',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wo',\n",
              "  'prefix_3': 'wom',\n",
              "  'prefix_4': 'wome',\n",
              "  'prev_word': 'and',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'men',\n",
              "  'suffix_4': 'omen',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'with',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wa',\n",
              "  'prefix_3': 'wal',\n",
              "  'prefix_4': 'walk',\n",
              "  'prev_word': 'women',\n",
              "  'suffix_1': 'k',\n",
              "  'suffix_2': 'lk',\n",
              "  'suffix_3': 'alk',\n",
              "  'suffix_4': 'walk',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'great',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wi',\n",
              "  'prefix_3': 'wit',\n",
              "  'prefix_4': 'with',\n",
              "  'prev_word': 'walk',\n",
              "  'suffix_1': 'h',\n",
              "  'suffix_2': 'th',\n",
              "  'suffix_3': 'ith',\n",
              "  'suffix_4': 'with',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'purpose',\n",
              "  'prefix_1': 'g',\n",
              "  'prefix_2': 'gr',\n",
              "  'prefix_3': 'gre',\n",
              "  'prefix_4': 'grea',\n",
              "  'prev_word': 'with',\n",
              "  'suffix_1': 't',\n",
              "  'suffix_2': 'at',\n",
              "  'suffix_3': 'eat',\n",
              "  'suffix_4': 'reat',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': ',',\n",
              "  'prefix_1': 'p',\n",
              "  'prefix_2': 'pu',\n",
              "  'prefix_3': 'pur',\n",
              "  'prefix_4': 'purp',\n",
              "  'prev_word': 'great',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'se',\n",
              "  'suffix_3': 'ose',\n",
              "  'suffix_4': 'pose',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '*-2',\n",
              "  'prefix_1': ',',\n",
              "  'prefix_2': ',',\n",
              "  'prefix_3': ',',\n",
              "  'prefix_4': ',',\n",
              "  'prev_word': 'purpose',\n",
              "  'suffix_1': ',',\n",
              "  'suffix_2': ',',\n",
              "  'suffix_3': ',',\n",
              "  'suffix_4': ',',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'noticing',\n",
              "  'prefix_1': '*',\n",
              "  'prefix_2': '*-',\n",
              "  'prefix_3': '*-2',\n",
              "  'prefix_4': '*-2',\n",
              "  'prev_word': ',',\n",
              "  'suffix_1': '2',\n",
              "  'suffix_2': '-2',\n",
              "  'suffix_3': '*-2',\n",
              "  'suffix_4': '*-2',\n",
              "  'word_has_hyphen': 1},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'one',\n",
              "  'prefix_1': 'n',\n",
              "  'prefix_2': 'no',\n",
              "  'prefix_3': 'not',\n",
              "  'prefix_4': 'noti',\n",
              "  'prev_word': '*-2',\n",
              "  'suffix_1': 'g',\n",
              "  'suffix_2': 'ng',\n",
              "  'suffix_3': 'ing',\n",
              "  'suffix_4': 'cing',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'another',\n",
              "  'prefix_1': 'o',\n",
              "  'prefix_2': 'on',\n",
              "  'prefix_3': 'one',\n",
              "  'prefix_4': 'one',\n",
              "  'prev_word': 'noticing',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'ne',\n",
              "  'suffix_3': 'one',\n",
              "  'suffix_4': 'one',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'only',\n",
              "  'prefix_1': 'a',\n",
              "  'prefix_2': 'an',\n",
              "  'prefix_3': 'ano',\n",
              "  'prefix_4': 'anot',\n",
              "  'prev_word': 'one',\n",
              "  'suffix_1': 'r',\n",
              "  'suffix_2': 'er',\n",
              "  'suffix_3': 'her',\n",
              "  'suffix_4': 'ther',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'when',\n",
              "  'prefix_1': 'o',\n",
              "  'prefix_2': 'on',\n",
              "  'prefix_3': 'onl',\n",
              "  'prefix_4': 'only',\n",
              "  'prev_word': 'another',\n",
              "  'suffix_1': 'y',\n",
              "  'suffix_2': 'ly',\n",
              "  'suffix_3': 'nly',\n",
              "  'suffix_4': 'only',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'they',\n",
              "  'prefix_1': 'w',\n",
              "  'prefix_2': 'wh',\n",
              "  'prefix_3': 'whe',\n",
              "  'prefix_4': 'when',\n",
              "  'prev_word': 'only',\n",
              "  'suffix_1': 'n',\n",
              "  'suffix_2': 'en',\n",
              "  'suffix_3': 'hen',\n",
              "  'suffix_4': 'when',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'jostle',\n",
              "  'prefix_1': 't',\n",
              "  'prefix_2': 'th',\n",
              "  'prefix_3': 'the',\n",
              "  'prefix_4': 'they',\n",
              "  'prev_word': 'when',\n",
              "  'suffix_1': 'y',\n",
              "  'suffix_2': 'ey',\n",
              "  'suffix_3': 'hey',\n",
              "  'suffix_4': 'they',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'for',\n",
              "  'prefix_1': 'j',\n",
              "  'prefix_2': 'jo',\n",
              "  'prefix_3': 'jos',\n",
              "  'prefix_4': 'jost',\n",
              "  'prev_word': 'they',\n",
              "  'suffix_1': 'e',\n",
              "  'suffix_2': 'le',\n",
              "  'suffix_3': 'tle',\n",
              "  'suffix_4': 'stle',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': 'cabs',\n",
              "  'prefix_1': 'f',\n",
              "  'prefix_2': 'fo',\n",
              "  'prefix_3': 'for',\n",
              "  'prefix_4': 'for',\n",
              "  'prev_word': 'jostle',\n",
              "  'suffix_1': 'r',\n",
              "  'suffix_2': 'or',\n",
              "  'suffix_3': 'for',\n",
              "  'suffix_4': 'for',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 0,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '*T*-1',\n",
              "  'prefix_1': 'c',\n",
              "  'prefix_2': 'ca',\n",
              "  'prefix_3': 'cab',\n",
              "  'prefix_4': 'cabs',\n",
              "  'prev_word': 'for',\n",
              "  'suffix_1': 's',\n",
              "  'suffix_2': 'bs',\n",
              "  'suffix_3': 'abs',\n",
              "  'suffix_4': 'cabs',\n",
              "  'word_has_hyphen': 0},\n",
              " {'is_alphanumeric': 1,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 0,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '.',\n",
              "  'prefix_1': '*',\n",
              "  'prefix_2': '*T',\n",
              "  'prefix_3': '*T*',\n",
              "  'prefix_4': '*T*-',\n",
              "  'prev_word': 'cabs',\n",
              "  'suffix_1': '1',\n",
              "  'suffix_2': '-1',\n",
              "  'suffix_3': '*-1',\n",
              "  'suffix_4': 'T*-1',\n",
              "  'word_has_hyphen': 1},\n",
              " {'is_alphanumeric': 0,\n",
              "  'is_complete_capital': 1,\n",
              "  'is_first_capital': 0,\n",
              "  'is_first_word': 0,\n",
              "  'is_last_word': 1,\n",
              "  'is_numeric': 0,\n",
              "  'next_word': '',\n",
              "  'prefix_1': '.',\n",
              "  'prefix_2': '.',\n",
              "  'prefix_3': '.',\n",
              "  'prefix_4': '.',\n",
              "  'prev_word': '*T*-1',\n",
              "  'suffix_1': '.',\n",
              "  'suffix_2': '.',\n",
              "  'suffix_3': '.',\n",
              "  'suffix_4': '.',\n",
              "  'word_has_hyphen': 0}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwpm6gTauhyd"
      },
      "source": [
        "CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. Among the various implementations of CRFs, this software provides following features.\n",
        "\n",
        "Fast training and tagging. The primary mission of this software is to train and use CRF models as fast as possible. See the benchmark result for more information.\n",
        "\n",
        "Simple data format for training and tagging. The data format is similar to those used in other machine learning tools; each line consists of a label and attributes (features) of an item, consequtive lines represent a sequence of items (an empty line denotes an end of item sequence). This means that users can design an arbitrary number of features for each item, which is impossible in CRF++."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxKXozhnurMA"
      },
      "source": [
        "State-of-the-art training methods. CRFsuite implements:\n",
        "\n",
        "***Limited-memory BFGS (L-BFGS) ***\n",
        "\n",
        "Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) method \n",
        "\n",
        "Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Averaged Perceptron \n",
        "\n",
        "Passive Aggressive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdGVPoLk4AHP",
        "outputId": "d441a8fa-daa1-4e15-fe00-84a72ee2e339"
      },
      "source": [
        "from sklearn_crfsuite import CRF\n",
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.01, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOOvZ-GW4bgA",
        "outputId": "95b19138-6216-43ed-8d59-0f41fa4545e1"
      },
      "source": [
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "y_pred=crf.predict(X_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_test, y_pred,average='weighted',labels=crf.classes_))\n",
        "print(\"F score on Training Data \")\n",
        "y_pred_train=crf.predict(X_train)\n",
        "metrics.flat_f1_score(y_train, y_pred_train,average='weighted',labels=crf.classes_)\n",
        "\n",
        "### Look at class wise score\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=crf.classes_, digits=3\n",
        "))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on Test Data \n",
            "0.9738471726864286\n",
            "F score on Training Data \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADP      0.979     0.985     0.982      1869\n",
            "        NOUN      0.966     0.977     0.972      5606\n",
            "        CONJ      0.994     0.994     0.994       480\n",
            "        VERB      0.964     0.960     0.962      2722\n",
            "         ADJ      0.911     0.874     0.892      1274\n",
            "           .      1.000     1.000     1.000      2354\n",
            "           X      1.000     0.997     0.998      1278\n",
            "         NUM      0.991     0.993     0.992       671\n",
            "         DET      0.994     0.995     0.994      1695\n",
            "         ADV      0.927     0.909     0.918       585\n",
            "        PRON      0.998     0.998     0.998       562\n",
            "         PRT      0.979     0.982     0.980       614\n",
            "\n",
            "    accuracy                          0.974     19710\n",
            "   macro avg      0.975     0.972     0.974     19710\n",
            "weighted avg      0.974     0.974     0.974     19710\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "demNDwqwSUAb",
        "outputId": "8abcaf78-e294-4007-ffbe-e81f9f315fbe"
      },
      "source": [
        "user_input=input(\"Enter the Sentence to find POS:\")\n",
        "def prepareData(user_input):\n",
        "    X,y=[],[]\n",
        "    for sentences in user_input:\n",
        "        X.append([features(untag(sentences), index) for index in range(len(sentences))])\n",
        "        y.append([tag for word,tag in sentences])\n",
        "    return X,y\n",
        "X_train,y_train=prepareData(train_set)\n",
        "X_test,y_test=prepareData(test_set)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Sentence to find POS:1.Coronaviruses (CoV) are a large family of viruses that cause illness ranging from the common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS-CoV) and Severe Acute Respiratory Syndrome (SARS-CoV). A novel coronavirus (nCoV) is a new strain that has not been previously identified in humans.  Coronaviruses are zoonotic, meaning they are transmitted between animals and people. Detailed investigations found that SARS-CoV was transmitted from civet cats to humans and MERS-CoV from dromedary camels to humans. Several known coronaviruses are circulating in animals that have not yet infected humans.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f6VNckfdkoC",
        "outputId": "2de511ed-ab62-4be7-d406-e4c624ccffe6"
      },
      "source": [
        "train_set, test_set = train_test_split(user_input,test_size=0.2,random_state=1234)\n",
        "print(\"Number of Sentences in Training Data \",len(train_set))\n",
        "print(\"Number of Sentences in Testing Data \",len(test_set))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences in Training Data  501\n",
            "Number of Sentences in Testing Data  126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDfXeX6xScFb",
        "outputId": "8c082059-a553-4efd-93de-5052a45edba1"
      },
      "source": [
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "y_pred=crf.predict(X_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_test, y_pred,average='weighted',labels=crf.classes_))\n",
        "print(\"F score on Training Data \")\n",
        "y_pred_train=crf.predict(X_train)\n",
        "metrics.flat_f1_score(y_train, y_pred_train,average='weighted',labels=crf.classes_)\n",
        "\n",
        "### Look at class wise score\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=crf.classes_, digits=3\n",
        "))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on Test Data \n",
            "0.9738471726864286\n",
            "F score on Training Data \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADP      0.979     0.985     0.982      1869\n",
            "        NOUN      0.966     0.977     0.972      5606\n",
            "        CONJ      0.994     0.994     0.994       480\n",
            "        VERB      0.964     0.960     0.962      2722\n",
            "         ADJ      0.911     0.874     0.892      1274\n",
            "           .      1.000     1.000     1.000      2354\n",
            "           X      1.000     0.997     0.998      1278\n",
            "         NUM      0.991     0.993     0.992       671\n",
            "         DET      0.994     0.995     0.994      1695\n",
            "         ADV      0.927     0.909     0.918       585\n",
            "        PRON      0.998     0.998     0.998       562\n",
            "         PRT      0.979     0.982     0.980       614\n",
            "\n",
            "    accuracy                          0.974     19710\n",
            "   macro avg      0.975     0.972     0.974     19710\n",
            "weighted avg      0.974     0.974     0.974     19710\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}