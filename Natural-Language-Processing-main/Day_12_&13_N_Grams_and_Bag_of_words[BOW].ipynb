{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day-12 &13 N-Grams and Bag-of-words[BOW].ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrEV5ZN/b8B3xfm999lmjc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-Sai-Kiran/Natural-Language-Processing/blob/main/Day_12_%2613_N_Grams_and_Bag_of_words%5BBOW%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What are n-grams?\n",
        "\n",
        "N-grams are **continuous sequences of words or symbols or tokens** in a document.\n",
        "\n",
        "In technical terms, they can be defined as the neighbouring sequences of items in a document. They come into play when we deal with text data in NLP(Natural Language Processing) tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "vPXkqegCC-Tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How are n-grams classified?"
      ],
      "metadata": {
        "id": "fO8W-8XvDE7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#n-Term\n",
        "\n",
        "1-Unigram\n",
        "\n",
        "2-Bigram\n",
        "\n",
        "3-Trigram\n",
        "\n",
        "n-n-gram"
      ],
      "metadata": {
        "id": "bMLqVi-WECS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AiE7Ju21ETS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DkfUYF1LIeW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "sentence = input(\"Enter the sentence: \")\n",
        "n = int(input(\"Enter the value of n: \"))\n",
        "n_grams = ngrams(sentence.split(), n)\n",
        "for grams in n_grams:\n",
        "    print(grams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp_0F380IenD",
        "outputId": "e465450e-bb61-4d12-e82c-21ace585b5a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence: Shami celebrated becoming the third fastest fast bowler to cross 200 Test wickets. Shami achieved the feat in 55 Test matches, behind former fast bowler Javagal Srinath who did so in 54 matches and all-round great Kapil Dev, who reached the mark in just 50 Tests.\n",
            "Enter the value of n: 5\n",
            "('Shami', 'celebrated', 'becoming', 'the', 'third')\n",
            "('celebrated', 'becoming', 'the', 'third', 'fastest')\n",
            "('becoming', 'the', 'third', 'fastest', 'fast')\n",
            "('the', 'third', 'fastest', 'fast', 'bowler')\n",
            "('third', 'fastest', 'fast', 'bowler', 'to')\n",
            "('fastest', 'fast', 'bowler', 'to', 'cross')\n",
            "('fast', 'bowler', 'to', 'cross', '200')\n",
            "('bowler', 'to', 'cross', '200', 'Test')\n",
            "('to', 'cross', '200', 'Test', 'wickets.')\n",
            "('cross', '200', 'Test', 'wickets.', 'Shami')\n",
            "('200', 'Test', 'wickets.', 'Shami', 'achieved')\n",
            "('Test', 'wickets.', 'Shami', 'achieved', 'the')\n",
            "('wickets.', 'Shami', 'achieved', 'the', 'feat')\n",
            "('Shami', 'achieved', 'the', 'feat', 'in')\n",
            "('achieved', 'the', 'feat', 'in', '55')\n",
            "('the', 'feat', 'in', '55', 'Test')\n",
            "('feat', 'in', '55', 'Test', 'matches,')\n",
            "('in', '55', 'Test', 'matches,', 'behind')\n",
            "('55', 'Test', 'matches,', 'behind', 'former')\n",
            "('Test', 'matches,', 'behind', 'former', 'fast')\n",
            "('matches,', 'behind', 'former', 'fast', 'bowler')\n",
            "('behind', 'former', 'fast', 'bowler', 'Javagal')\n",
            "('former', 'fast', 'bowler', 'Javagal', 'Srinath')\n",
            "('fast', 'bowler', 'Javagal', 'Srinath', 'who')\n",
            "('bowler', 'Javagal', 'Srinath', 'who', 'did')\n",
            "('Javagal', 'Srinath', 'who', 'did', 'so')\n",
            "('Srinath', 'who', 'did', 'so', 'in')\n",
            "('who', 'did', 'so', 'in', '54')\n",
            "('did', 'so', 'in', '54', 'matches')\n",
            "('so', 'in', '54', 'matches', 'and')\n",
            "('in', '54', 'matches', 'and', 'all-round')\n",
            "('54', 'matches', 'and', 'all-round', 'great')\n",
            "('matches', 'and', 'all-round', 'great', 'Kapil')\n",
            "('and', 'all-round', 'great', 'Kapil', 'Dev,')\n",
            "('all-round', 'great', 'Kapil', 'Dev,', 'who')\n",
            "('great', 'Kapil', 'Dev,', 'who', 'reached')\n",
            "('Kapil', 'Dev,', 'who', 'reached', 'the')\n",
            "('Dev,', 'who', 'reached', 'the', 'mark')\n",
            "('who', 'reached', 'the', 'mark', 'in')\n",
            "('reached', 'the', 'mark', 'in', 'just')\n",
            "('the', 'mark', 'in', 'just', '50')\n",
            "('mark', 'in', 'just', '50', 'Tests.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding the Bag of Words Model"
      ],
      "metadata": {
        "id": "RmCXwYmwLOZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bag-of-words model, or BoW for short, **is a way of extracting features from text for use in modeling**, such as with machine learning algorithms.\n",
        "\n",
        "The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "\n",
        "**1.A vocabulary of known words.**\n",
        "\n",
        "**2.A measure of the presence of known words.**"
      ],
      "metadata": {
        "id": "vN9VM8RcLWas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required modules\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict \n",
        " \n",
        "#Sample text corpus\n",
        "data = ['She loves pizza, pizza is delicious.','She is a good person.','good people are the best.']\n",
        " \n",
        "#clean the corpus.\n",
        "sentences = []\n",
        "vocab = []\n",
        "for sent in data:\n",
        "    x = word_tokenize(sent)\n",
        "    sentence = [w.lower() for w in x if w.isalpha() ]\n",
        "    sentences.append(sentence)\n",
        "    for word in sentence:\n",
        "        if word not in vocab:\n",
        "            vocab.append(word)\n",
        " \n",
        "#number of words in the vocab\n",
        "len_vector = len(vocab)\n",
        "len_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca--82G9Lh9-",
        "outputId": "bc07f52f-9716-4ca8-f654-b8366eb91ba3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Index dictionary to assign an index to each word in vocabulary\n",
        "index_word = {}\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    index_word[word] = i \n",
        "    i += 1"
      ],
      "metadata": {
        "id": "Y_LjeRoXInLB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(sent):\n",
        "    count_dict = defaultdict(int)\n",
        "    vec = np.zeros(len_vector)\n",
        "    for item in sent:\n",
        "        count_dict[item] += 1\n",
        "    for key,item in count_dict.items():\n",
        "        vec[index_word[key]] = item\n",
        "    return vec"
      ],
      "metadata": {
        "id": "a7ysuCYWL4p9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = bag_of_words(sentences[0])\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoVjXD_qL81u",
        "outputId": "9d364919-d715-49f3-dec5-92e9e8091c77"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitations of Bag-of-Words**\n",
        "\n",
        "Even though the Bag of Words model is super simple to implement, it still has some shortcomings.\n",
        "\n",
        "**Sparsity: BOW models create sparse vectors which increase space complexities and also makes it difficult for our prediction algorithm to learn.**\n",
        "\n",
        "**Meaning: The order of the sequence is not preserved in the BOW model hence the context and meaning of a sentence can be lost.**"
      ],
      "metadata": {
        "id": "HpBsU5WDOqMM"
      }
    }
  ]
}